{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sk learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n",
    "\n",
    "\n",
    "# from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "# google_model_path= 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "google_model_path = './GoogleNews-vectors-negative300.bin.gz'\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(google_model_path, binary=True, limit=10 ** 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('../jigsaw-toxic-comment-classification-challenge/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_original.copy(deep = True)\n",
    "df=df.drop(columns = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \n",
       "0        0       0       0              0  \n",
       "1        0       0       0              0  \n",
       "2        0       0       0              0  \n",
       "3        0       0       0              0  \n",
       "4        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_queries = [\n",
    "#                 '@([A-Za-z0-9_]+)', #Usernames for twitter\n",
    "#                'rt\\s:\\s', #Retweets marker\n",
    "                 '(https?:\\s?\\/\\s?\\/)(\\s)?(www\\.)?(\\s?)(\\w+\\.)*([\\w\\-\\s]+\\/)*([\\w-]+)\\/?' # Hyperlinks\n",
    "                ]\n",
    "\n",
    "df['text'] = df['comment_text'].replace(regex_queries,'',regex = True)\n",
    "# Remove symbols\n",
    "df['text'].replace('[^\\w\\s]|_','', regex = True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Daww He matches this background colour Im seem...</td>\n",
       "      <td>[daww, he, matches, this, background, colour, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man Im really not trying to edit war Its j...</td>\n",
       "      <td>[hey, man, im, really, not, trying, to, edit, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nMore\\nI cant make any real suggestions on im...</td>\n",
       "      <td>[, more, i, cant, make, any, real, suggestions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "\n",
       "                                                text  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  Daww He matches this background colour Im seem...   \n",
       "2  Hey man Im really not trying to edit war Its j...   \n",
       "3  \\nMore\\nI cant make any real suggestions on im...   \n",
       "4  You sir are my hero Any chance you remember wh...   \n",
       "\n",
       "                                      text_tokenized  \n",
       "0  [explanation, why, the, edits, made, under, my...  \n",
       "1  [daww, he, matches, this, background, colour, ...  \n",
       "2  [hey, man, im, really, not, trying, to, edit, ...  \n",
       "3  [, more, i, cant, make, any, real, suggestions...  \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "df['text_tokenized'] = df['text'].apply(lambda x: tokenization(x.lower()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopWords]\n",
    "    return text\n",
    "    \n",
    "df['text_nonstop'] = df['text_tokenized'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text if len(word) < 30]  # the length restriction prevents  \n",
    "                                                                # long strings from overflowing recursion used in stemmer\n",
    "    return text\n",
    "\n",
    "df['text_stemmed'] = df['text_nonstop'].apply(lambda x: stemming(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_nonstop</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>[explan, edit, made, usernam, hardcor, metalli...</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Daww He matches this background colour Im seem...</td>\n",
       "      <td>[daww, he, matches, this, background, colour, ...</td>\n",
       "      <td>[daww, matches, background, colour, im, seemin...</td>\n",
       "      <td>[daww, match, background, colour, im, seemingl...</td>\n",
       "      <td>[daww, match, background, colour, im, seemingl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man Im really not trying to edit war Its j...</td>\n",
       "      <td>[hey, man, im, really, not, trying, to, edit, ...</td>\n",
       "      <td>[hey, man, im, really, trying, edit, war, guy,...</td>\n",
       "      <td>[hey, man, im, realli, tri, edit, war, guy, co...</td>\n",
       "      <td>[hey, man, im, really, trying, edit, war, guy,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nMore\\nI cant make any real suggestions on im...</td>\n",
       "      <td>[, more, i, cant, make, any, real, suggestions...</td>\n",
       "      <td>[, cant, make, real, suggestions, improvement,...</td>\n",
       "      <td>[, cant, make, real, suggest, improv, wonder, ...</td>\n",
       "      <td>[, cant, make, real, suggestion, improvement, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "      <td>[sir, hero, chance, remember, page, thats]</td>\n",
       "      <td>[sir, hero, chanc, rememb, page, that]</td>\n",
       "      <td>[sir, hero, chance, remember, page, thats]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "\n",
       "                                                text  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  Daww He matches this background colour Im seem...   \n",
       "2  Hey man Im really not trying to edit war Its j...   \n",
       "3  \\nMore\\nI cant make any real suggestions on im...   \n",
       "4  You sir are my hero Any chance you remember wh...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [daww, he, matches, this, background, colour, ...   \n",
       "2  [hey, man, im, really, not, trying, to, edit, ...   \n",
       "3  [, more, i, cant, make, any, real, suggestions...   \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...   \n",
       "\n",
       "                                        text_nonstop  \\\n",
       "0  [explanation, edits, made, username, hardcore,...   \n",
       "1  [daww, matches, background, colour, im, seemin...   \n",
       "2  [hey, man, im, really, trying, edit, war, guy,...   \n",
       "3  [, cant, make, real, suggestions, improvement,...   \n",
       "4         [sir, hero, chance, remember, page, thats]   \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0  [explan, edit, made, usernam, hardcor, metalli...   \n",
       "1  [daww, match, background, colour, im, seemingl...   \n",
       "2  [hey, man, im, realli, tri, edit, war, guy, co...   \n",
       "3  [, cant, make, real, suggest, improv, wonder, ...   \n",
       "4             [sir, hero, chanc, rememb, page, that]   \n",
       "\n",
       "                                         text_lemmed  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [daww, match, background, colour, im, seemingl...  \n",
       "2  [hey, man, im, really, trying, edit, war, guy,...  \n",
       "3  [, cant, make, real, suggestion, improvement, ...  \n",
       "4         [sir, hero, chance, remember, page, thats]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text if len(word) < 30] \n",
    "    return text\n",
    "\n",
    "df['text_lemmed'] = df['text_nonstop'].apply(lambda x: lemmatizer(x))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_words(text):\n",
    "        text = [model.get_vector(word) for word in text if word in model.vocab]\n",
    "        \n",
    "        return text\n",
    "df['text_vectorized'] = df['text_lemmed'].apply(lambda x: vectorize_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_nonstop</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmed</th>\n",
       "      <th>text_vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>[explan, edit, made, usernam, hardcor, metalli...</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>[[-0.07080078, -0.3671875, -0.008972168, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Daww He matches this background colour Im seem...</td>\n",
       "      <td>[daww, he, matches, this, background, colour, ...</td>\n",
       "      <td>[daww, matches, background, colour, im, seemin...</td>\n",
       "      <td>[daww, match, background, colour, im, seemingl...</td>\n",
       "      <td>[daww, match, background, colour, im, seemingl...</td>\n",
       "      <td>[[-0.15527344, 0.025024414, 0.064941406, -0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man Im really not trying to edit war Its j...</td>\n",
       "      <td>[hey, man, im, really, not, trying, to, edit, ...</td>\n",
       "      <td>[hey, man, im, really, trying, edit, war, guy,...</td>\n",
       "      <td>[hey, man, im, realli, tri, edit, war, guy, co...</td>\n",
       "      <td>[hey, man, im, really, trying, edit, war, guy,...</td>\n",
       "      <td>[[0.021606445, 0.021728516, 0.05029297, 0.3378...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nMore\\nI cant make any real suggestions on im...</td>\n",
       "      <td>[, more, i, cant, make, any, real, suggestions...</td>\n",
       "      <td>[, cant, make, real, suggestions, improvement,...</td>\n",
       "      <td>[, cant, make, real, suggest, improv, wonder, ...</td>\n",
       "      <td>[, cant, make, real, suggestion, improvement, ...</td>\n",
       "      <td>[[0.20410156, -0.030395508, 0.0006866455, 0.34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "      <td>[sir, hero, chance, remember, page, thats]</td>\n",
       "      <td>[sir, hero, chanc, rememb, page, that]</td>\n",
       "      <td>[sir, hero, chance, remember, page, thats]</td>\n",
       "      <td>[[0.083496094, 0.045410156, 0.25, 0.47070312, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "\n",
       "                                                text  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  Daww He matches this background colour Im seem...   \n",
       "2  Hey man Im really not trying to edit war Its j...   \n",
       "3  \\nMore\\nI cant make any real suggestions on im...   \n",
       "4  You sir are my hero Any chance you remember wh...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [daww, he, matches, this, background, colour, ...   \n",
       "2  [hey, man, im, really, not, trying, to, edit, ...   \n",
       "3  [, more, i, cant, make, any, real, suggestions...   \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...   \n",
       "\n",
       "                                        text_nonstop  \\\n",
       "0  [explanation, edits, made, username, hardcore,...   \n",
       "1  [daww, matches, background, colour, im, seemin...   \n",
       "2  [hey, man, im, really, trying, edit, war, guy,...   \n",
       "3  [, cant, make, real, suggestions, improvement,...   \n",
       "4         [sir, hero, chance, remember, page, thats]   \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0  [explan, edit, made, usernam, hardcor, metalli...   \n",
       "1  [daww, match, background, colour, im, seemingl...   \n",
       "2  [hey, man, im, realli, tri, edit, war, guy, co...   \n",
       "3  [, cant, make, real, suggest, improv, wonder, ...   \n",
       "4             [sir, hero, chanc, rememb, page, that]   \n",
       "\n",
       "                                         text_lemmed  \\\n",
       "0  [explanation, edits, made, username, hardcore,...   \n",
       "1  [daww, match, background, colour, im, seemingl...   \n",
       "2  [hey, man, im, really, trying, edit, war, guy,...   \n",
       "3  [, cant, make, real, suggestion, improvement, ...   \n",
       "4         [sir, hero, chance, remember, page, thats]   \n",
       "\n",
       "                                     text_vectorized  \n",
       "0  [[-0.07080078, -0.3671875, -0.008972168, -0.07...  \n",
       "1  [[-0.15527344, 0.025024414, 0.064941406, -0.12...  \n",
       "2  [[0.021606445, 0.021728516, 0.05029297, 0.3378...  \n",
       "3  [[0.20410156, -0.030395508, 0.0006866455, 0.34...  \n",
       "4  [[0.083496094, 0.045410156, 0.25, 0.47070312, ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df['text_lemmed'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_nonstop</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmed</th>\n",
       "      <th>text_vectorized</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32143</th>\n",
       "      <td>PIG PIG PIG PIG PIG PIG PIG PIG PIG PIG PIG PI...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PIG PIG PIG PIG PIG PIG PIG PIG PIG PIG PIG PI...</td>\n",
       "      <td>[pig, pig, pig, pig, pig, pig, pig, pig, pig, ...</td>\n",
       "      <td>[pig, pig, pig, pig, pig, pig, pig, pig, pig, ...</td>\n",
       "      <td>[pig, pig, pig, pig, pig, pig, pig, pig, pig, ...</td>\n",
       "      <td>[pig, pig, pig, pig, pig, pig, pig, pig, pig, ...</td>\n",
       "      <td>[[-0.084472656, -0.07714844, -0.042236328, 0.2...</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61793</th>\n",
       "      <td>OH NOES OH NOES OH NOES OH NOES OH NOES OH NOE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OH NOES OH NOES OH NOES OH NOES OH NOES OH NOE...</td>\n",
       "      <td>[oh, noes, oh, noes, oh, noes, oh, noes, oh, n...</td>\n",
       "      <td>[oh, noes, oh, noes, oh, noes, oh, noes, oh, n...</td>\n",
       "      <td>[oh, noe, oh, noe, oh, noe, oh, noe, oh, noe, ...</td>\n",
       "      <td>[oh, no, oh, no, oh, no, oh, no, oh, no, oh, n...</td>\n",
       "      <td>[[0.07128906, 0.06933594, 0.075683594, 0.20800...</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76598</th>\n",
       "      <td>DIE FAG DIE FAG DIE FAG DIE FAG DIE FAG DIE FA...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DIE FAG DIE FAG DIE FAG DIE FAG DIE FAG DIE FA...</td>\n",
       "      <td>[die, fag, die, fag, die, fag, die, fag, die, ...</td>\n",
       "      <td>[die, fag, die, fag, die, fag, die, fag, die, ...</td>\n",
       "      <td>[die, fag, die, fag, die, fag, die, fag, die, ...</td>\n",
       "      <td>[die, fag, die, fag, die, fag, die, fag, die, ...</td>\n",
       "      <td>[[0.100097656, 0.17578125, 0.043701172, 0.4062...</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150039</th>\n",
       "      <td>LOL LOL LOL LOL LOL LOL LOL LOL LOL LOL LOL LO...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LOL LOL LOL LOL LOL LOL LOL LOL LOL LOL LOL LO...</td>\n",
       "      <td>[lol, lol, lol, lol, lol, lol, lol, lol, lol, ...</td>\n",
       "      <td>[lol, lol, lol, lol, lol, lol, lol, lol, lol, ...</td>\n",
       "      <td>[lol, lol, lol, lol, lol, lol, lol, lol, lol, ...</td>\n",
       "      <td>[lol, lol, lol, lol, lol, lol, lol, lol, lol, ...</td>\n",
       "      <td>[[-0.23535156, -0.049560547, 0.08203125, 0.341...</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic  \\\n",
       "32143   PIG PIG PIG PIG PIG PIG PIG PIG PIG PIG PIG PI...      1   \n",
       "61793   OH NOES OH NOES OH NOES OH NOES OH NOES OH NOE...      0   \n",
       "76598   DIE FAG DIE FAG DIE FAG DIE FAG DIE FAG DIE FA...      1   \n",
       "150039  LOL LOL LOL LOL LOL LOL LOL LOL LOL LOL LOL LO...      0   \n",
       "\n",
       "        severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "32143              0        0       0       0              0   \n",
       "61793              0        0       0       0              0   \n",
       "76598              0        0       0       0              0   \n",
       "150039             0        0       0       0              0   \n",
       "\n",
       "                                                     text  \\\n",
       "32143   PIG PIG PIG PIG PIG PIG PIG PIG PIG PIG PIG PI...   \n",
       "61793   OH NOES OH NOES OH NOES OH NOES OH NOES OH NOE...   \n",
       "76598   DIE FAG DIE FAG DIE FAG DIE FAG DIE FAG DIE FA...   \n",
       "150039  LOL LOL LOL LOL LOL LOL LOL LOL LOL LOL LOL LO...   \n",
       "\n",
       "                                           text_tokenized  \\\n",
       "32143   [pig, pig, pig, pig, pig, pig, pig, pig, pig, ...   \n",
       "61793   [oh, noes, oh, noes, oh, noes, oh, noes, oh, n...   \n",
       "76598   [die, fag, die, fag, die, fag, die, fag, die, ...   \n",
       "150039  [lol, lol, lol, lol, lol, lol, lol, lol, lol, ...   \n",
       "\n",
       "                                             text_nonstop  \\\n",
       "32143   [pig, pig, pig, pig, pig, pig, pig, pig, pig, ...   \n",
       "61793   [oh, noes, oh, noes, oh, noes, oh, noes, oh, n...   \n",
       "76598   [die, fag, die, fag, die, fag, die, fag, die, ...   \n",
       "150039  [lol, lol, lol, lol, lol, lol, lol, lol, lol, ...   \n",
       "\n",
       "                                             text_stemmed  \\\n",
       "32143   [pig, pig, pig, pig, pig, pig, pig, pig, pig, ...   \n",
       "61793   [oh, noe, oh, noe, oh, noe, oh, noe, oh, noe, ...   \n",
       "76598   [die, fag, die, fag, die, fag, die, fag, die, ...   \n",
       "150039  [lol, lol, lol, lol, lol, lol, lol, lol, lol, ...   \n",
       "\n",
       "                                              text_lemmed  \\\n",
       "32143   [pig, pig, pig, pig, pig, pig, pig, pig, pig, ...   \n",
       "61793   [oh, no, oh, no, oh, no, oh, no, oh, no, oh, n...   \n",
       "76598   [die, fag, die, fag, die, fag, die, fag, die, ...   \n",
       "150039  [lol, lol, lol, lol, lol, lol, lol, lol, lol, ...   \n",
       "\n",
       "                                          text_vectorized  num_words  \n",
       "32143   [[-0.084472656, -0.07714844, -0.042236328, 0.2...       1250  \n",
       "61793   [[0.07128906, 0.06933594, 0.075683594, 0.20800...       1250  \n",
       "76598   [[0.100097656, 0.17578125, 0.043701172, 0.4062...       1250  \n",
       "150039  [[-0.23535156, -0.049560547, 0.08203125, 0.341...       1250  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['num_words'] == df['num_words'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.29559e+05, 1.92630e+04, 5.45500e+03, 1.98200e+03, 9.83000e+02,\n",
       "        6.88000e+02, 5.63000e+02, 4.60000e+02, 4.11000e+02, 9.40000e+01]),\n",
       " array([  0. ,  49.9,  99.8, 149.7, 199.6, 249.5, 299.4, 349.3, 399.2,\n",
       "        449.1, 499. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['num_words'].loc[df['num_words'] < 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'countVector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f0fa04ff4f2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# countVector = countVectorizer.fit_transform(df['text'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# print('{} Number of Tweets has {} words'.format(countVector.shape[0], countVector.shape[1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcount_vect_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountVector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'countVector' is not defined"
     ]
    }
   ],
   "source": [
    "# def clean_text(text):\n",
    "#     text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "#     text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "#     tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "#     text = [ps.stem(word) for word in tokens if word not in stopWords and len(word) < 30]  # remove stopwords and stemming\n",
    "#     return text\n",
    "# countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "# countVector = countVectorizer.fit_transform(df['text'])\n",
    "# print('{} Number of Tweets has {} words'.format(countVector.shape[0], countVector.shape[1]))\n",
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(df['text_vectorized'], maxlen=500, value = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-931765772341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad each comment first up to 100, then flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model building\n",
    "\n",
    "\n",
    "\n",
    "# NN = Sequential()\n",
    "# # Node, input dimensions\n",
    "# NN.add(Dense(100,  input_dim = X_train.shape)) # need feature input dim (28x28) for first hidden layer\n",
    "# NN.add(Activation('relu'))\n",
    "\n",
    "# NN.add(Dense(20))\n",
    "# NN.add(Activation('relu'))\n",
    "\n",
    "# NN.add(Dense(10)) # note we would typically use higher dim than this for last hidden layer\n",
    "# NN.add(Activation('relu', name = '2D_layer')) # naming this layer so we can extract it later\n",
    "\n",
    "# # Output layer\n",
    "\n",
    "# NN.add(Dense(6))\n",
    "# NN.add(Activation('sigmoid'))\n",
    "\n",
    "# # NN.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# # For multi label (Classes are not mutually exclusive)\n",
    "# NN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# NN.fit(X_train, y_train, epochs=20, batch_size=512, verbose=1) # track progress as we fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keras example code\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Activation\n",
    "# from keras.layers import Embedding\n",
    "# from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "# from keras.datasets import imdb\n",
    "\n",
    "# # set parameters:\n",
    "# max_features = 5000\n",
    "# maxlen = 400\n",
    "# batch_size = 32\n",
    "# embedding_dims = 50\n",
    "# filters = 250\n",
    "# kernel_size = 3\n",
    "# hidden_dims = 250\n",
    "# epochs = 2\n",
    "\n",
    "# print('Loading data...')\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# print(len(x_train), 'train sequences')\n",
    "# print(len(x_test), 'test sequences')\n",
    "\n",
    "# print('Pad sequences (samples x time)')\n",
    "# x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "# x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print('x_test shape:', x_test.shape)\n",
    "\n",
    "# print('Build model...')\n",
    "# model = Sequential()\n",
    "\n",
    "# # we start off with an efficient embedding layer which maps\n",
    "# # our vocab indices into embedding_dims dimensions\n",
    "# model.add(Embedding(max_features,\n",
    "#                     embedding_dims,\n",
    "#                     input_length=maxlen))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# # we add a Convolution1D, which will learn filters\n",
    "# # word group filters of size filter_length:\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# # we use max pooling:\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# # We add a vanilla hidden layer:\n",
    "# model.add(Dense(hidden_dims))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['toxic'] = 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
