{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('../jigsaw-toxic-comment-classification-challenge/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../jigsaw-toxic-comment-classification-challenge/test.csv').fillna(' ')\n",
    "\n",
    "list_sentences_train = train['comment_text']\n",
    "list_sentences_test = test['comment_text']\n",
    "all_text = pd.concat([list_sentences_train, list_sentences_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Explanation\\nWhy the edits made under my usern...\n",
       "1         D'aww! He matches this background colour I'm s...\n",
       "2         Hey man, I'm really not trying to edit war. It...\n",
       "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4         You, sir, are my hero. Any chance you remember...\n",
       "5         \"\\n\\nCongratulations from me as well, use the ...\n",
       "6              COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
       "7         Your vandalism to the Matt Shirvington article...\n",
       "8         Sorry if the word 'nonsense' was offensive to ...\n",
       "9         alignment on this subject and which are contra...\n",
       "10        \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...\n",
       "11        bbq \\n\\nbe a man and lets discuss it-maybe ove...\n",
       "12        Hey... what is it..\\n@ | talk .\\nWhat is it......\n",
       "13        Before you start throwing accusations and warn...\n",
       "14        Oh, and the girl above started her arguments w...\n",
       "15        \"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...\n",
       "16        Bye! \\n\\nDon't look, come or think of comming ...\n",
       "17         REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski\n",
       "18        The Mitsurugi point made no sense - why not ar...\n",
       "19        Don't mean to bother you \\n\\nI see that you're...\n",
       "20        \"\\n\\n Regarding your recent edits \\n\\nOnce aga...\n",
       "21        \"\\nGood to know. About me, yeah, I'm studying ...\n",
       "22        \"\\n\\n Snowflakes are NOT always symmetrical! \\...\n",
       "23        \"\\n\\n The Signpost: 24 September 2012 \\n\\n Rea...\n",
       "24        \"\\n\\nRe-considering 1st paragraph edit?\\nI don...\n",
       "25        Radial symmetry \\n\\nSeveral now extinct lineag...\n",
       "26        There's no need to apologize. A Wikipedia arti...\n",
       "27        Yes, because the mother of the child in the ca...\n",
       "28        \"\\nOk. But it will take a bit of work but I ca...\n",
       "29        \"== A barnstar for you! ==\\n\\n  The Real Life ...\n",
       "                                ...                        \n",
       "153134    \" \\n\\n == Same coffee shop? == \\n\\n My memory ...\n",
       "153135    SO many things wrong with that viewpoint - fro...\n",
       "153136    \" \\n\\n Unless we have an article for some othe...\n",
       "153137    Hannah and Maddie are soooooo awesome and are ...\n",
       "153138    :::no problem, I tagged it and cleaned it out....\n",
       "153139    :PS I've just looked at the history of this ar...\n",
       "153140    \"== Your edit to Maungaturoto == \\n Please don...\n",
       "153141    :If you wish to contest the prod, please remov...\n",
       "153142    Balancing the two approaches to psychiatry ( b...\n",
       "153143                                   Ah, suck my balls.\n",
       "153144    == Your name mentioned == \\n Hi, I just though...\n",
       "153145    I've just discovered yet another list: List of...\n",
       "153146    ==Wikiproject Video Games assessment== \\n I do...\n",
       "153147    ::Consensus for ruining Wikipedia? I think tha...\n",
       "153148    == DAP ?  == \\n\\n What's point with DAP ?! Naz...\n",
       "153149    shut down the mexican border withought looking...\n",
       "153150    :Jerome, I see you never got around to this…! ...\n",
       "153151    ==Lucky bastard== \\n http://wikimediafoundatio...\n",
       "153152    ==WTF== \\n It's no longer a redlink.  Now what...\n",
       "153153    \" \\n\\n ==\"\"Illness\"\" no shit== \\n Just for the...\n",
       "153154    ==shame on you all!!!== \\n\\n You want to speak...\n",
       "153155    MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...\n",
       "153156    \" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...\n",
       "153157    :Disagree. Soviet railways need their own arti...\n",
       "153158    This idiot can't even use proper grammar when ...\n",
       "153159    . \\n i totally agree, this stuff is nothing bu...\n",
       "153160    == Throw from out field to home plate. == \\n\\n...\n",
       "153161    \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n",
       "153162    \" \\n\\n == \"\"One of the founding nations of the...\n",
       "153163    \" \\n :::Stop already. Your bullshit is not wel...\n",
       "Name: comment_text, Length: 312735, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ａ': 'a',\n",
       " 'ｂ': 'b',\n",
       " 'ｃ': 'c',\n",
       " 'ｄ': 'd',\n",
       " 'ｅ': 'e',\n",
       " 'ｆ': 'f',\n",
       " 'ｇ': 'g',\n",
       " 'ｈ': 'h',\n",
       " 'ｉ': 'i',\n",
       " 'ｊ': 'j',\n",
       " 'ｋ': 'k',\n",
       " 'ｌ': 'l',\n",
       " 'ｍ': 'm',\n",
       " 'ｎ': 'n',\n",
       " 'ｏ': 'o',\n",
       " 'ｐ': 'p',\n",
       " 'ｑ': 'q',\n",
       " 'ｒ': 'r',\n",
       " 'ｓ': 's',\n",
       " 'ｔ': 't',\n",
       " 'ｕ': 'u',\n",
       " 'ｖ': 'v',\n",
       " 'ｗ': 'w',\n",
       " 'ｘ': 'x',\n",
       " 'ｙ': 'y',\n",
       " 'ｚ': 'z',\n",
       " 'Ａ': 'a',\n",
       " 'Ｂ': 'b',\n",
       " 'Ｃ': 'c',\n",
       " 'Ｄ': 'd',\n",
       " 'Ｅ': 'e',\n",
       " 'Ｆ': 'f',\n",
       " 'Ｇ': 'g',\n",
       " 'Ｈ': 'h',\n",
       " 'Ｉ': 'i',\n",
       " 'Ｊ': 'j',\n",
       " 'Ｋ': 'k',\n",
       " 'Ｌ': 'l',\n",
       " 'Ｍ': 'm',\n",
       " 'Ｎ': 'n',\n",
       " 'Ｏ': 'o',\n",
       " 'Ｐ': 'p',\n",
       " 'Ｑ': 'q',\n",
       " 'Ｒ': 'r',\n",
       " 'Ｓ': 's',\n",
       " 'Ｔ': 't',\n",
       " 'Ｕ': 'u',\n",
       " 'Ｖ': 'v',\n",
       " 'Ｗ': 'w',\n",
       " 'Ｘ': 'x',\n",
       " 'Ｙ': 'y',\n",
       " 'Ｚ': 'z',\n",
       " '？': ' ?',\n",
       " '！': ' !',\n",
       " '．': ' .',\n",
       " '＝': ' =',\n",
       " '＋': ' +',\n",
       " '－': ' -',\n",
       " '１': '1',\n",
       " '２': '2',\n",
       " '３': '3',\n",
       " '４': '4',\n",
       " '５': '5',\n",
       " '６': '6',\n",
       " '７': '7',\n",
       " '８': '8',\n",
       " '９': '9',\n",
       " '０': '0'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "cl_path = './cleaning/clean_letters.txt'\n",
    "clean_word_dict = {}\n",
    "with open(cl_path, 'r', encoding='utf-8') as cl:\n",
    "    for line in cl:\n",
    "        line = line.strip('\\n')\n",
    "        typo, correct = line.split(',')\n",
    "        clean_word_dict[typo] = correct\n",
    "\n",
    "def clean_word(text):\n",
    "    replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "    special_character_removal = re.compile(r'[^a-z\\d ]', re.IGNORECASE)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n",
    "\n",
    "    for typo, correct in clean_word_dict.items():\n",
    "        text = re.sub(typo, \" \" + correct + \" \", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"i’m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = replace_numbers.sub('', text)\n",
    "    return text\n",
    "\n",
    "train_text = []\n",
    "test_text = []\n",
    "for text in list_sentences_train:\n",
    "    train_text.append(clean_word(text))\n",
    "    \n",
    "for text in list_sentences_test:\n",
    "    test_text.append(clean_word(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=50000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "# char_vectorizer = TfidfVectorizer(\n",
    "#     sublinear_tf=True,\n",
    "#     strip_accents='unicode',\n",
    "#     analyzer='char',\n",
    "#     ngram_range=(1, 6),\n",
    "#     max_features=30000)\n",
    "# char_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=True,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_char_features = char_vectorizer.transform(train_text)\n",
    "# test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9563520283043244\n",
      "CV score for class severe_toxic is 0.9374788299258515\n",
      "CV score for class obscene is 0.9779372902677976\n",
      "CV score for class threat is 0.8677795057582118\n",
      "CV score for class insult is 0.9630752821396903\n",
      "CV score for class identity_hate is 0.8979753961074951\n"
     ]
    }
   ],
   "source": [
    "train_features = train_word_features\n",
    "test_features = test_word_features\n",
    "losses = []\n",
    "predictions = {'id': test['id']}\n",
    "model_dict = dict()\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = ExtraTreesClassifier(n_estimators=30)\n",
    "    \n",
    "    cv_loss = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "    \n",
    "    classifier.fit(train_features, train_target)\n",
    "    model_dict[class_name] = classifier\n",
    "    predictions[class_name] = classifier.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file \n",
    "model_dict_imported = joblib.load('models/models.p') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def raw_chat_to_model_input(raw_input_string):\n",
    "    \n",
    "    cleaned_text = []\n",
    "    for text in [raw_input_string]:\n",
    "        cleaned_text.append(clean_word(text))\n",
    "    #print(cleaned_text)\n",
    "    return word_vectorizer.transform(cleaned_text)\n",
    "\n",
    "    \n",
    "def predict_toxicity(raw_input_string):\n",
    "    model_input = raw_chat_to_model_input(raw_input_string)\n",
    "    results = []\n",
    "    for key,model in model_dict.items():\n",
    "        results.append(round(model.predict_proba(model_input)[0,1],4))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuck you this is bullshit gay queer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_chat_to_model_input(chat_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuck you this is bullshit gay queer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06666667, 0.93333333]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict['toxic'].predict_proba(raw_chat_to_model_input(chat_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic 0.7\n",
      "severe_toxic 0.1333\n",
      "obscene 0.0703\n",
      "threat 0.0\n",
      "insult 0.0685\n",
      "identity_hate 0.1667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chat_input = 'trash is garbage'\n",
    "\n",
    "output_list = [list(model_dict.keys()),predict_toxicity(chat_input)]\n",
    "for index in range(len(output_list[0])):\n",
    "    print(output_list[0][index],output_list[1][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "severe_toxic\n",
      "obscene\n",
      "threat\n",
      "insult\n",
      "identity_hate\n"
     ]
    }
   ],
   "source": [
    "for key in model_dict:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "forsen_chat = joblib.load('./chat_logs/forsen_chat.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14019, 4)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forsen_chat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "1         [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "2         [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "3         [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "4            [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "5         [0.0333, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "6         [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "7      [0.1333, 0.0, 0.0333, 0.0, 0.0667, 0.0]\n",
       "8         [0.091, 0.0, 0.091, 0.0, 0.027, 0.0]\n",
       "9         [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "10     [0.2333, 0.0, 0.0333, 0.0333, 0.0, 0.0]\n",
       "11           [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "12        [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "13        [0.091, 0.0, 0.091, 0.0, 0.027, 0.0]\n",
       "14       [0.027, 0.0, 0.0919, 0.0, 0.027, 0.0]\n",
       "15           [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "16        [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "17           [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "18        [0.0333, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "19     [0.2333, 0.0, 0.0333, 0.0333, 0.0, 0.0]\n",
       "20     [0.0333, 0.0, 0.0, 0.0, 0.0333, 0.1333]\n",
       "21        [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "22           [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "23        [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "24        [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "25       [0.027, 0.0, 0.027, 0.0, 0.0595, 0.0]\n",
       "26     [0.2667, 0.2, 0.0, 0.0667, 0.1333, 0.0]\n",
       "27           [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "28        [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "29           [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "                        ...                   \n",
       "970    [0.1081, 0.0, 0.0045, 0.0, 0.0405, 0.0]\n",
       "971          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "972       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "973       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "974       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "975          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "976     [0.0261, 0.0, 0.027, 0.0, 0.0595, 0.0]\n",
       "977       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "978          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "979    [0.0081, 0.0, 0.0081, 0.0, 0.0045, 0.0]\n",
       "980       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "981       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "982       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "983       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "984       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "985          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "986          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "987       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "988    [0.2342, 0.0, 0.1667, 0.0, 0.2, 0.0333]\n",
       "989       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "990          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "991       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "992       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "993       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "994       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "995       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "996       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "997       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "998          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "999    [0.027, 0.0, 0.027, 0.0, 0.027, 0.0667]\n",
       "Name: message, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forsen_chat['message'][0:1000].apply(lambda msg : predict_toxicity(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
