{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('../jigsaw-toxic-comment-classification-challenge/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../jigsaw-toxic-comment-classification-challenge/test.csv').fillna(' ')\n",
    "\n",
    "list_sentences_train = train['comment_text']\n",
    "list_sentences_test = test['comment_text']\n",
    "all_text = pd.concat([list_sentences_train, list_sentences_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "cl_path = './cleaning/clean_letters.txt'\n",
    "clean_word_dict = {}\n",
    "with open(cl_path, 'r', encoding='utf-8') as cl:\n",
    "    for line in cl:\n",
    "        line = line.strip('\\n')\n",
    "        typo, correct = line.split(',')\n",
    "        clean_word_dict[typo] = correct\n",
    "\n",
    "def clean_word(text):\n",
    "    replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "    special_character_removal = re.compile(r'[^a-z\\d ]', re.IGNORECASE)\n",
    "\n",
    "    text = text.lower()\n",
    "    # Replace links\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n",
    "\n",
    "    for typo, correct in clean_word_dict.items():\n",
    "        text = re.sub(typo, \" \" + correct + \" \", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"iâ€™m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = replace_numbers.sub('', text)\n",
    "    return text\n",
    "\n",
    "train_text = []\n",
    "test_text = []\n",
    "for text in list_sentences_train:\n",
    "    train_text.append(clean_word(text))\n",
    "    \n",
    "for text in list_sentences_test:\n",
    "    test_text.append(clean_word(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=20000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "# char_vectorizer = TfidfVectorizer(\n",
    "#     sublinear_tf=True,\n",
    "#     strip_accents='unicode',\n",
    "#     analyzer='char',\n",
    "#     ngram_range=(1, 6),\n",
    "#     max_features=30000)\n",
    "# char_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=20000,\n",
       "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents='unicode',\n",
       "                sublinear_tf=True, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_char_features = char_vectorizer.transform(train_text)\n",
    "# test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9540573174056127\n",
      "CV score for class severe_toxic is 0.940742843803584\n",
      "CV score for class obscene is 0.9755727809885308\n",
      "CV score for class threat is 0.8772669637141552\n",
      "CV score for class insult is 0.9584597405987246\n",
      "CV score for class identity_hate is 0.8851517034144708\n"
     ]
    }
   ],
   "source": [
    "train_features = train_word_features\n",
    "test_features = test_word_features\n",
    "losses = []\n",
    "predictions = {'id': test['id']}\n",
    "model_dict = dict()\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = ExtraTreesClassifier(n_estimators=30)\n",
    "    \n",
    "    cv_loss = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "    \n",
    "    classifier.fit(train_features, train_target)\n",
    "    model_dict[class_name] = classifier\n",
    "    predictions[class_name] = classifier.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_dict_imported' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d4037887a0be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_dict_imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_dict_imported' is not defined"
     ]
    }
   ],
   "source": [
    "model_dict_imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator ExtraTreeClassifier from version 0.20.3 when using version 0.21.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator ExtraTreesClassifier from version 0.20.3 when using version 0.21.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the file \n",
    "model_dict_imported = joblib.load('models/models.p') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3246608\r\n",
      "drwxr-xr-x  14 jchow  staff   448B May 21 18:02 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   8 jchow  staff   256B May 20 14:38 \u001b[34m..\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  13 jchow  staff   416B May 20 11:46 \u001b[34m.git\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 jchow  staff    54B May 18 21:29 .gitignore\r\n",
      "drwxr-xr-x   7 jchow  staff   224B May 18 21:16 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 jchow  staff   432K May 17 10:20 EDA_tokenized.ipynb\r\n",
      "-rw-r--r--   1 jchow  staff    57K May 17 17:00 EDA_wiki_dataset.ipynb\r\n",
      "-rw-r--r--   1 jchow  staff    19K May 14 13:48 ExporatoryDataAnalysis.ipynb\r\n",
      "-rw-r--r--   1 jchow  staff   1.5G Mar  5  2015 GoogleNews-vectors-negative300.bin.gz\r\n",
      "-rw-r--r--   1 jchow  staff    28K May 21 18:02 Kaggle_Inspired.ipynb\r\n",
      "-rw-r--r--   1 jchow  staff   201B May 20 11:41 README.md\r\n",
      "drwxr-xr-x@ 16 jchow  staff   512B May 19 00:35 \u001b[34mchat_logs\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 jchow  staff    96B May 18 20:55 \u001b[34mcleaning\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   5 jchow  staff   160B May 21 18:00 \u001b[34mmodels\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/models_compressed.p']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(model_dict_imported,'models/models_compressed.p',compress = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_vectorizer.p']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(word_vectorizer, 'word_vectorizer.p') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def raw_chat_to_model_input(raw_input_string):\n",
    "    \n",
    "    cleaned_text = []\n",
    "    for text in [raw_input_string]:\n",
    "        cleaned_text.append(clean_word(text))\n",
    "    #print(cleaned_text)\n",
    "    return word_vectorizer.transform(cleaned_text)\n",
    "\n",
    "    \n",
    "def predict_toxicity(raw_input_string):\n",
    "    model_input = raw_chat_to_model_input(raw_input_string)\n",
    "    results = []\n",
    "    for key,model in model_dict.items():\n",
    "        results.append(round(model.predict_proba(model_input)[0,1],4))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_chat_to_model_input(chat_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuck you this is bullshit gay queer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06666667, 0.93333333]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict['toxic'].predict_proba(raw_chat_to_model_input(chat_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       " 'severe_toxic': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       " 'obscene': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       " 'threat': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       " 'insult': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       " 'identity_hate': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False)}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic 0.7\n",
      "severe_toxic 0.1333\n",
      "obscene 0.0703\n",
      "threat 0.0\n",
      "insult 0.0685\n",
      "identity_hate 0.1667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chat_input = 'trash is garbage'\n",
    "\n",
    "output_list = [list(model_dict.keys()),predict_toxicity(chat_input)]\n",
    "for index in range(len(output_list[0])):\n",
    "    print(output_list[0][index],output_list[1][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "severe_toxic\n",
      "obscene\n",
      "threat\n",
      "insult\n",
      "identity_hate\n"
     ]
    }
   ],
   "source": [
    "for key in model_dict:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "forsen_chat = joblib.load('./chat_logs/forsen_chat.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14019, 4)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forsen_chat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "1         [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "2         [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "3         [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "4            [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "5         [0.0333, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "6         [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "7      [0.1333, 0.0, 0.0333, 0.0, 0.0667, 0.0]\n",
       "8         [0.091, 0.0, 0.091, 0.0, 0.027, 0.0]\n",
       "9         [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "10     [0.2333, 0.0, 0.0333, 0.0333, 0.0, 0.0]\n",
       "11           [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "12        [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "13        [0.091, 0.0, 0.091, 0.0, 0.027, 0.0]\n",
       "14       [0.027, 0.0, 0.0919, 0.0, 0.027, 0.0]\n",
       "15           [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "16        [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "17           [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "18        [0.0333, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "19     [0.2333, 0.0, 0.0333, 0.0333, 0.0, 0.0]\n",
       "20     [0.0333, 0.0, 0.0, 0.0, 0.0333, 0.1333]\n",
       "21        [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "22           [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "23        [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "24        [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "25       [0.027, 0.0, 0.027, 0.0, 0.0595, 0.0]\n",
       "26     [0.2667, 0.2, 0.0, 0.0667, 0.1333, 0.0]\n",
       "27           [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "28        [0.0, 0.0, 0.0333, 0.0, 0.0333, 0.0]\n",
       "29           [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "                        ...                   \n",
       "970    [0.1081, 0.0, 0.0045, 0.0, 0.0405, 0.0]\n",
       "971          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "972       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "973       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "974       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "975          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "976     [0.0261, 0.0, 0.027, 0.0, 0.0595, 0.0]\n",
       "977       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "978          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "979    [0.0081, 0.0, 0.0081, 0.0, 0.0045, 0.0]\n",
       "980       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "981       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "982       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "983       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "984       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "985          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "986          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "987       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "988    [0.2342, 0.0, 0.1667, 0.0, 0.2, 0.0333]\n",
       "989       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "990          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "991       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "992       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "993       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "994       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "995       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "996       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "997       [0.027, 0.0, 0.027, 0.0, 0.027, 0.0]\n",
       "998          [0.0, 0.0, 0.0333, 0.0, 0.0, 0.0]\n",
       "999    [0.027, 0.0, 0.027, 0.0, 0.027, 0.0667]\n",
       "Name: message, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forsen_chat['message'][0:1000].apply(lambda msg : predict_toxicity(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_dict.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'toxic', 'prob': 0.7},\n",
       " {'name': 'identity_hate', 'prob': 0.1667},\n",
       " {'name': 'severe_toxic', 'prob': 0.1333},\n",
       " {'name': 'obscene', 'prob': 0.0703},\n",
       " {'name': 'insult', 'prob': 0.0685},\n",
       " {'name': 'threat', 'prob': 0.0}]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pred_probs = predict_toxicity(chat_input)\n",
    "\n",
    "probs = [{'name': list(model_dict.keys())[index], 'prob': pred_probs[index]}\n",
    "         for index in np.argsort(pred_probs)[::-1]]\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
